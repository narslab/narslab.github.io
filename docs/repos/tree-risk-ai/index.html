<h1 id="tree-risk-ai">tree-risk-ai</h1>
<p>Artificial Intelligence for Tree Failure Identification and Risk Quantification</p>

<h2 id="summary">Summary</h2>
<p>The code and models in this repository implement convolutional neural network (CNN) models to predict tree likelihood of failure categories from a given input image. The categories are:</p>
<ul>
  <li>Improbable: failure unlikely either during normal or extreme weather conditions</li>
  <li>Possible: failure expected under extreme weather conditions; but unlikely during normal weather conditions</li>
  <li>Probable: failure expected under normal weather conditions within a given time frame
Original input images are 3024 x 4032 pixels. We assess the performance of an optimized CNN using 64-pixel, 128-pixel and 224-pixel inputs (after data augmentation expands samples from 525 images to 2525 images).
We also evaluate performance under four classification scenarios (investigating how various category groupings impact classifier performance):
Pr_Im: {Probable, Improbable}; 2 classes
PrPo_Im: {Probable + Possible, Improbable}; 2 classes
Pr_PoIm: {Probable, Possible + Improbable}; 2 classes
Pr_Po_Im: {Probable, Possible, Improbable}; 3 classes</li>
</ul>

<h2 id="step-1-label-input-data">Step 1: Label input data</h2>
<p>Inputs are images (currently 3024 x 4032 pixels). These are currently saved locally and not accessible on the remote. Email the collaborators for data access. To perform labeling, run <code class="language-plaintext highlighter-rouge">label-image-files.py</code>. The user must specify the path to the raw images (<code class="language-plaintext highlighter-rouge">RAW_IMAGE_DIR</code>). The current framework assumes the raw images are housed in <code class="language-plaintext highlighter-rouge">data/raw/Pictures for AI</code>.</p>

<h2 id="step-2-preprocess-images">Step 2: Preprocess images</h2>
<p>In this step (<code class="language-plaintext highlighter-rouge">preprocess-images.py</code>), we perform image resizing and data augmentation (random cropping, horizontal flipping - probability of 50%). The user can specify the expansion factor for the original set of images. For instance, there are 525 images in the original dataset. if an expansion factor of 5 is specified for the preprocessing function, then the final augmented set will contain 2525 images. Finally, image training sets are generated for 4 classification scenarios and for user-specified resolutions, e.g. 64 x 64 px, 128 x 128 px, etc. One-hot-vector encoding is also performed. Each set of images and labels are saved as an array of tuples in a binary <code class="language-plaintext highlighter-rouge">.npy</code> file. The <code class="language-plaintext highlighter-rouge">preprocess-images.py</code> script also includes a <code class="language-plaintext highlighter-rouge">plotProcessedImages()</code> function that generates a specified number of randomly chosen input images for each scenario.</p>

<p>The user can also plot selected processed images using the functions in the <code class="language-plaintext highlighter-rouge">plot-processed-images.py</code> script. To explore all the processed images in a matrix plot, use <code class="language-plaintext highlighter-rouge">exploreProcessedImages()</code>. Figure 2 in the manuscript was generated using the <code class="language-plaintext highlighter-rouge">plotSelectedProcessedImages()</code> function.</p>

<h2 id="step-3-cnn-hyperparameter-optimization">Step 3: CNN hyperparameter optimization</h2>
<p>We use the <code class="language-plaintext highlighter-rouge">Hyperband</code> function from <code class="language-plaintext highlighter-rouge">keras.tuner</code> to optimize the following parameters in our convolutional neural network: kernel size of first convolutional layer, units in the 2 dense layers, their respective dropout rates and activation functions. The routine is carried out in <code class="language-plaintext highlighter-rouge">cnn-hyperparameter-optimization.py</code>. The search is performed for 12 cases (3 resolutions and 4 classification scenarios).</p>
<ul>
  <li>The results are tabulated via <code class="language-plaintext highlighter-rouge">tabulate-optimal-hyperparameters.py</code> (which generates the CSV files used to create Table 4 in the manuscript).</li>
</ul>

<h2 id="step-4-sensitivity-tests">Step 4: Sensitivity tests</h2>
<p>In <code class="language-plaintext highlighter-rouge">resolution-scenario-sensitivity.py</code>, the function <code class="language-plaintext highlighter-rouge">testResolutionScenarioPerformance()</code> conducts CNN model fitting for each combination of resolution and scenario as specified by the user in <code class="language-plaintext highlighter-rouge">RESOLUTION_LIST</code> and <code class="language-plaintext highlighter-rouge">SCENARIO_LIST</code> respectively. This is done via k-fold cross-validation. Validation metrics of macro-average precision, recall and $F_1$ are also implemented. Model histories are saved for each trial.</p>

<p>Tabulation and visualization summaries of the results are implemented in <code class="language-plaintext highlighter-rouge">senstivity-analysis.ipynb</code>.</p>
<ul>
  <li>Figure 4 in the manuscript is generated using <code class="language-plaintext highlighter-rouge">plotMeanAccuracyLoss()</code>.</li>
  <li>Figure 5 is generated using <code class="language-plaintext highlighter-rouge">plotSummaryValidationMetrics()</code></li>
</ul>

<p>Furthermore, we aggregate performance statistics in <code class="language-plaintext highlighter-rouge">senstivity-analysis.ipynb</code> and performance Welchâ€™s tests to determine  if there are significant differences in outcomes.</p>
<ul>
  <li>The function <code class="language-plaintext highlighter-rouge">getScenarioResolutionMeanPerformance()</code> generates Table 6.</li>
  <li>The function <code class="language-plaintext highlighter-rouge">resolutionPerformanceComparisonStats()</code> generates Table 7.</li>
  <li>The function <code class="language-plaintext highlighter-rouge">scenarioPerformanceComparisonStats()</code> generates Table 8.</li>
</ul>

<h2 id="step-5-detailed-cnn-performance-analysis">Step 5: Detailed CNN performance analysis</h2>
<p>In <code class="language-plaintext highlighter-rouge">cnn-performance.py</code>, we define the function <code class="language-plaintext highlighter-rouge">trainModelWithDetailedMetrics()</code> which implements CNN model-fitting, along with sklearn classification metrics, including a confusion matrix, for a given resolution/scenario instance. The loss and performance results are visualized in the <code class="language-plaintext highlighter-rouge">plot-cnn-performance.ipynb</code> notebook, using the function <code class="language-plaintext highlighter-rouge">plotCNNPerformanceMetrics()</code>.</p>
<ul>
  <li>Figure 6 in the manuscript is generated via <code class="language-plaintext highlighter-rouge">plotCNNPerformanceMetrics()</code>.</li>
  <li>Figure 7 is based on the confusion matrices saved from running <code class="language-plaintext highlighter-rouge">getScenarioModelPerformance()</code>, which in turns runs <code class="language-plaintext highlighter-rouge">trainModelWithDetailedMetrics()</code>.
The trained model is saved to <code class="language-plaintext highlighter-rouge">results/models/</code>.</li>
</ul>

<h2 id="step-6-cnn-visualization-and-inference-in-progress">Step 6: CNN Visualization and Inference (in progress)</h2>
<p>We implement GradCAM and saliency maps to understand how the CNN classifies an image. This is done using <code class="language-plaintext highlighter-rouge">plotGradCAM()</code> and <code class="language-plaintext highlighter-rouge">plotSaliency()</code> in <code class="language-plaintext highlighter-rouge">cnn-visualization.ipynb</code>. A prior trained model is loaded (e.g. <code class="language-plaintext highlighter-rouge">m = models.load_model('../../results/models/opt-cnn-Pr_Im-128-px/model')</code>) and used as an input to either of the functions mentioned.</p>

<p>Please note: Function and classes that are used in two or more scripts are housed in <code class="language-plaintext highlighter-rouge">helpers.py</code></p>
